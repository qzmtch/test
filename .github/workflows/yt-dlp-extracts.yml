name: Build yt-dlp extracts

on:
  workflow_dispatch:
    inputs:
      force:
        description: 'Force run even if upstream tag unchanged'
        type: boolean
        default: false
  schedule:
    - cron: "0 0 * * *" # раз в сутки (UTC)

concurrency:
  group: ytdlp-extracts
  cancel-in-progress: false

jobs:
  extract:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout this repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Detect upstream build and decide
        id: check
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          FORCE: ${{ github.event.inputs.force }}
        run: |
          set -e

          # 1) Получаем последний релиз master-builds
          UPSTREAM_TAG=$(gh api repos/yt-dlp/yt-dlp-master-builds/releases/latest --jq .tag_name)
          UPSTREAM_BODY=$(gh api repos/yt-dlp/yt-dlp-master-builds/releases/latest --jq .body)
          echo "Latest upstream tag: $UPSTREAM_TAG"

          # 2) Пытаемся достать SHA коммита yt-dlp из описания релиза
          CAND=$(printf "%s" "$UPSTREAM_BODY" | grep -Eoi "(commit|yt-dlp)\s*[:#]?\s*([0-9a-f]{7,40})" | head -n1 || true)
          YTDLP_REF=$(printf "%s" "$CAND" | grep -Eo "[0-9a-f]{7,40}" || true)
          if [ -z "$YTDLP_REF" ]; then
            YTDLP_REF="master"
            echo "No commit found in release body; using yt-dlp ref: master"
          else
            echo "Using yt-dlp ref (commit): $YTDLP_REF"
          fi

          echo "UPSTREAM_TAG=$UPSTREAM_TAG" >> $GITHUB_ENV
          echo "YTDLP_REF=$YTDLP_REF" >> $GITHUB_ENV

          # 3) Проверяем что уже публиковали для этого тега (если не force)
          TARGET_TAG="ytdlp-extracts"
          if gh release view "$TARGET_TAG" >/dev/null 2>&1; then
            gh release download "$TARGET_TAG" -p source_tag.txt -D . >/dev/null 2>&1 || true
          fi
          PREV_TAG="$(cat source_tag.txt 2>/dev/null || echo '')"
          echo "Previous processed tag: ${PREV_TAG:-<none>}"

          # 4) Решение о запуске
          if [ "${FORCE}" = "true" ] || [ "${FORCE}" = "1" ]; then
            echo "Force run requested via workflow_dispatch."
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [ "$PREV_TAG" = "$UPSTREAM_TAG" ]; then
            echo "No new upstream tag ($UPSTREAM_TAG). Skipping."
            echo "should_run=false" >> $GITHUB_OUTPUT
          else
            echo "New upstream tag detected: $UPSTREAM_TAG"
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi

      - name: Checkout yt-dlp at detected ref
        if: steps.check.outputs.should_run == 'true'
        uses: actions/checkout@v4
        with:
          repository: yt-dlp/yt-dlp
          ref: ${{ env.YTDLP_REF }}
          path: vendor/yt-dlp
          fetch-depth: 1

      - name: Extract _VALID_URL, _EMBED_REGEX, test urls and domains
        if: steps.check.outputs.should_run == 'true'
        run: |
          python - <<'PY'
          import sys, importlib, inspect, re
          from urllib.parse import urlparse
          from pathlib import Path

          root = Path('vendor/yt-dlp')
          sys.path.insert(0, str(root))

          regexes = set()
          embed_regexes = set()
          urls = set()
          domains = set()

          from yt_dlp.extractor.common import InfoExtractor
          extractor_dir = root / 'yt_dlp' / 'extractor'

          Pattern = getattr(re, "Pattern", type(re.compile("")))

          def add_regex(val, dest):
            if not val:
              return
            if isinstance(val, str):
              dest.add(val)
            elif isinstance(val, Pattern):
              dest.add(val.pattern)
            elif isinstance(val, (list, tuple, set)):
              for v in val:
                add_regex(v, dest)

          for p in extractor_dir.glob('*.py'):
            if p.name in ('__init__.py', '_extractors.py'):
              continue
            modname = f"yt_dlp.extractor.{p.stem}"
            try:
              m = importlib.import_module(modname)
            except Exception as e:
              print(f"[WARN] skip {modname}: {e}")
              continue

            for name, obj in m.__dict__.items():
              if inspect.isclass(obj) and issubclass(obj, InfoExtractor) and obj is not InfoExtractor:
                add_regex(getattr(obj, '_VALID_URL', None), regexes)
                add_regex(getattr(obj, '_EMBED_REGEX', None), embed_regexes)

                tests = []
                _tests = getattr(obj, '_TESTS', None)
                if isinstance(_tests, (list, tuple)):
                  tests.extend(_tests)
                _test = getattr(obj, '_TEST', None)
                if isinstance(_test, dict):
                  tests.append(_test)

                for t in tests:
                  if isinstance(t, dict):
                    u = t.get('url')
                    if isinstance(u, str) and u:
                      urls.add(u)

          # домены из test_urls
          for u in urls:
            try:
              host = urlparse(u).hostname
            except Exception:
              host = None
            if not host:
              continue
            host = host.lower().strip('.')
            if host.startswith('www.'):
              host = host[4:]
            domains.add(host)

          Path('artifacts').mkdir(exist_ok=True)

          def write_sorted(name, data):
            with open(f'artifacts/{name}', 'w', encoding='utf-8') as f:
              for s in sorted(data):
                f.write(s + '\n')

          write_sorted('valid_url_regexes.txt', regexes)
          write_sorted('embed_regexes.txt', embed_regexes)
          write_sorted('test_urls.txt', urls)
          write_sorted('test_domains.txt', domains)

          print(f"Wrote {len(regexes)} _VALID_URL, {len(embed_regexes)} _EMBED_REGEX, {len(urls)} test urls, {len(domains)} domains")
          PY

      - name: Write source tag file
        if: steps.check.outputs.should_run == 'true'
        run: |
          mkdir -p artifacts
          printf "%s\n" "$UPSTREAM_TAG" > artifacts/source_tag.txt

      - name: Publish to single tag/release
        if: steps.check.outputs.should_run == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TAG="ytdlp-extracts"
          TITLE="yt-dlp extracts: _VALID_URL, _EMBED_REGEX, test urls and domains"
          NOTES="Auto-updated from yt-dlp-master-builds ${UPSTREAM_TAG} (yt-dlp ref: ${YTDLP_REF})"

          gh release view "$TAG" >/dev/null 2>&1 || gh release create "$TAG" --title "$TITLE" --notes "$NOTES"
          gh release edit "$TAG" --title "$TITLE" --notes "$NOTES"

          gh release upload "$TAG" \
            artifacts/valid_url_regexes.txt \
            artifacts/embed_regexes.txt \
            artifacts/test_urls.txt \
            artifacts/test_domains.txt \
            artifacts/source_tag.txt \
            --clobber
